{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=images/pyladiesatx.jpeg align=\"left\" width=\"12%\"></div>\n",
    "\n",
    "<img src=images/spark-logo-hd.png align=\"right\" width=\"12%\"></div>\n",
    "<img src=images/python-logo-notext.png align=\"right\" width=\"6%\"></div>\n",
    "\n",
    "\n",
    "<h1 align='center'>Intro to PySpark</h1>\n",
    "<h3 align='center'>June 20, 2016 -- PyLadies ATX</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Environment Setup\n",
    "We'll use the free Databricks Comunity Edition platform to run our Spark jobs: \n",
    "1. Use Google Chrome browser (Firefox should also work, but not Internet Explorer, Safari, etc.)\n",
    "2. Sign up for the Community Edition account here: https://databricks.com/try-databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Or, feel free to use a local installation of Spark, etc. If Spark isn't already installed on your machine it can take up to an hour to download and build from source locally (there are also pre-built versions that would be faster to set up):\n",
    "1. Download: http://spark.apache.org/downloads.html\n",
    "2. Open: `../spark1.6.1/README.md`\n",
    "3. Build: `../spark1.6.1/build/mvn -DskipTests clean package`\n",
    "    - On my laptop, time to build was ~30 mins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Q. Before I get started, I'm just curious, if I could see a show of hands, how many of you have used Spark before?  \n",
    "Q. How many of your are regular Python users?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview\n",
    "- Spark: What and Why?\n",
    "- PySpark API\n",
    "- Examples:\n",
    "    - Word Count\n",
    "    - Logistic Regression\n",
    "    - Clickstream\n",
    "- References and Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark: What and Why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So I'll just briefly give an introduction to what is Spark, for those of you who may be new to it.\n",
    "\n",
    "Spark is a fast and expressive cluster computing system for doing Big Data computation. It's good for iterative tasks, for doing big batch processing, and for interactive data exploration. \n",
    "\n",
    "And it's compatible with Hadoop-supported file systems and data formats (HDFS, S3, SequenceFile, ...), so you can use it with your existing data and deploy it on your existing clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Spark improves efficiency through in-memory computing primitives and general computation graphs.\n",
    "\n",
    "Spark improves usability through rich APIs in Scala, Python, and Java, and an interactive shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is Spark?\n",
    "In contrast to distributed shared memory systems where if you want fault taulerance you have to checkpoint the memory and roll back. RDDs - if you lost a partition of it you can reconstruct it through lineage.\n",
    "\n",
    "Especially helpful for iterative algorithms\n",
    "- linear regression\n",
    "- logistic regression\n",
    "\n",
    "RDDs do not need to be materialized at all time - they're lazily computed.\n",
    "\n",
    "Programmers can control 2 aspects of RDDs: caching and partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    ">\"Although current frameworks provide numerous abstractions for accessing a clusterâ€™s computational resources, they lack abstractions for leveraging distributed memory. This makes them inefficient for an important class of emerging applications: those that reuse intermediate results across multiple computations. Data reuse is common in many <font color='blue'>iterative</font> machine learning and graph algorithms, including PageRank, K-means clustering, and logistic regression. Another compelling use case is <font color='blue'>interactive</font> data mining, where a user runs multiple ad-hoc queries on the same subset of the data. Unfortunately, in most current frameworks, the only way to reuse data between computations (e.g., between two MapReduce jobs) is to write it to an external stable storage system, e.g., a distributed file system. This incurs substantial overheads due to data replication, disk I/O, and serialization, which can dominate application execution times.\"\n",
    "\n",
    "- Zaharia et al., \"Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing,\" *In NSDI '12*, April 2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark vs MapReduce vs MPI vs ...\n",
    "- [MapReduce](https://en.wikipedia.org/wiki/MapReduce) --> [Hadoop](http://hadoop.apache.org/): heavily used in business computing\n",
    "- [Message Passing Interface (MPI)](https://en.wikipedia.org/wiki/Message_Passing_Interface) --> [MVAPICH](http://mvapich.cse.ohio-state.edu/): heavily used in scientific computing\n",
    "- [Spark](http://spark.apache.org/): good for iterative tasks, big batch processing, interactive data exploration\n",
    "\n",
    "**Note: The difference between Spark and MapReduce is that you work by partition and not by element. A partition is just a chunk of our data - some number of elements. You can have multiple partitions on one worker node.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark Architecture\n",
    "- Spark Driver and Workers\n",
    "- SparkContext (replaced by SparkSession in version 2.X)\n",
    "\n",
    "<img src=images/cluster-overview.png align=\"center\" width=\"50%\"></div>\n",
    "\n",
    "<h4 align='right'>https://spark.apache.org/docs/1.1.0/cluster-overview.html</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark (version 1.X) Programming Concepts\n",
    "- ****SparkContext****: entry point to Spark functions\n",
    "    - `parallelize(c, numSlices=None)`\n",
    "- ****Resilient Distributed Datasets (RDDs)****:\n",
    "    - Immutable, distributed collections of objects\n",
    "    - Can be cached in memory for fast reuse\n",
    "- ****Operations on RDDs****:\n",
    "    - *Transformations*: define a new RDD (map, join, ...)\n",
    "    - *Actions*: return or output a result (count, save, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark Data Interfaces (versions 1.X and 2.X)\n",
    "\n",
    "There are several key interfaces that you should understand when you go to use Spark.\n",
    "\n",
    "-   ****The Dataset****\n",
    "    -   The Dataset is Apache Spark's newest distributed collection and can be considered a combination of DataFrames and RDDs. It provides the typed interface that is available in RDDs while providing a lot of conveniences of DataFrames. It will be the core abstraction going forward.\n",
    "-   ****The DataFrame****\n",
    "    -   The DataFrame is collection of distributed `Row` types. These provide a flexible interface and are similar in concept to the DataFrames you may be familiar with in python (pandas) as well as in the R language.\n",
    "-   ****The RDD (Resilient Distributed Dataset)****\n",
    "    -   Apache Spark's first abstraction was the RDD or Resilient Distributed Dataset. Essentially it is an interface to a sequence of data objects that consist of one or more types that are located across a variety of machines in a cluster. RDD's can be created in a variety of ways and are the \"lowest level\" API available to the user. While this is the original data structure made available, new users should focus on Datasets as those will be supersets of the current RDD functionality.\n",
    "\n",
    "*(slide taken from \"Introduction to Apache Spark on Databricks\" notebook)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The basic unit of abstraction in Spark to represent your data is an RDD, or a Resilient Distributed Dataset. It's an immutable, partitioned collection of objects.\n",
    "\n",
    "You can create a bunch of different sources, parallelize your data from a text file - it's this big partitioned collection of objects. And the way that you express your computation is by performing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformations\n",
    "- Transform one RDD to another RDD\n",
    "- Yields a new RDD\n",
    "\n",
    "| Transformation | Description | Type\\* |\n",
    "| :------:  | :-----------: | :-----: |\n",
    "| `map(func)`     | Apply a function over each element | Narrow |\n",
    "| `flatMap(func)` | Map then flatten output | Narrow |\n",
    "| `filter(func)`  | Keep only elements where function is `True` | Narrow |\n",
    "| `sample(withReplacement, fraction, seed)` | Return a sampled subset of this RDD. | Narrow |\n",
    "| `groupByKey(k, v)` | extension to be used for dest files. | Wide |\n",
    "| `reduceByKey(func)` | extension to be used for dest files. | Wide |\n",
    "\n",
    "\\* **Narrow** transformations are local to each node and don't imply transfering information across the network. **Wide** transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=images/narrow_wide_transformations.png align=\"center\" width=\"50%\"></div>\n",
    "\n",
    "<h4 align='right'>https://dzone.com/articles/big-data-processing-spark</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Actions\n",
    "- Return or output a result\n",
    "\n",
    "| Action | Description | Try it Out\\*|\n",
    "| :------:  | :-----------:| :---: |\n",
    "| `collect()`     | Return a list that contains all of the elements in this RDD. | `sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()` |\n",
    "| `count()`  | Return the number of elements in this RDD. | `sc.parallelize([2, 3, 4]).count()` |\n",
    "| `saveAsTextFile(path)` | Save this RDD as a text file, using string representations of elements. | `sc.parallelize(['', 'foo', '', 'bar', ''])\\ .saveAsTextFile(\"/FileStore/foo-bar.txt\")])`|\n",
    "| `first()`    | Return the first element in this RDD. | `sc.parallelize([2, 3, 4]).first()` |\n",
    "| `take(num)`    | Take the first num elements of the RDD. | `sc.parallelize([2, 3, 4, 5, 6]).take(2)` |\n",
    "\n",
    "\\* Let's try some simple jobs:\n",
    "1. Go to your databricks Workspace and create a new directory within your Users directory called \"2016-06-20-pyladies-pyspark\" \n",
    "2. Create a notebook called \"0-Introduction\"  within this directory\n",
    "3. Type or copy/paste lines of code into separate cells and run them (you will be prompted to launch a cluster) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Try a few more with transformations *and* actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.groupByKey().mapValues(len).collect())\n",
    "sorted(rdd.groupByKey().mapValues(list).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Example: Log Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"hdfs://...\")\n",
    "errors = lines.filter(_.startsWith(\"ERROR\"))\n",
    "messages = errors.map(_.split('\\t')(2))\n",
    "\n",
    "messages.filter(_.contains(\"foo\")).count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The computation is expressed declaratively and nothing actually takes place until calling `count` at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark API\n",
    "We'll focus on learning the main API calls needed for running Spark jobs through examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is PySpark?\n",
    "- Write Spark jobs in Python\n",
    "- Run interactive jobs in the shell\n",
    "- Supports C extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Core classes:\n",
    "#### pyspark.SparkContext\n",
    "\n",
    "Main entry point for Spark functionality.\n",
    "\n",
    "#### pyspark.RDD\n",
    "\n",
    "A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
    "\n",
    "#### pyspark.streaming.StreamingContext\n",
    "\n",
    "Main entry point for Spark Streaming functionality.\n",
    "\n",
    "#### pyspark.streaming.DStream\n",
    "\n",
    "A Discretized Stream (DStream), the basic abstraction in Spark Streaming.\n",
    "\n",
    "#### pyspark.sql.SQLContext\n",
    "\n",
    "Main entry point for DataFrame and SQL functionality.\n",
    "\n",
    "#### pyspark.sql.DataFrame\n",
    "\n",
    "A distributed collection of data grouped into named columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why use PySpark?\n",
    "- If you already know Python\n",
    "- Can use Spark in tandem with your favorite Python libraries\n",
    "- If you don't need Python libraries, maybe just write code in Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example 1: Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "\n",
    "sc = SparkContext(...)\n",
    "lines = sc.textFile(sys.argv[2], 1)\n",
    "counts = lines.flatMap(lambda x: x.split(' ') \\\n",
    "                      .map(lambda x: (x, 1)) \\\n",
    "                      .reduceByKey(lambda x, y: x + y))\n",
    "\n",
    "for (word, count) in counts.collect():\n",
    "    print \"%s : %i\" % (word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get initial RDD from the context\n",
    "file = spark.textFile(\"hdfs://...\")\n",
    "# Three consecutive transformation of the RDD\n",
    "counts = file.flatMap(lambda line: line.split(\" \"))\n",
    "             .map(lambda word: (word, 1))\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "# Materialize the RDD using an action\n",
    "counts.saveAsTextFile(\"hdfs://...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Every record of this DataFrame contains the label and\n",
    "# features represented by a vector.\n",
    "df = sqlContext.createDataFrame(data, [\"label\", \"features\"])\n",
    "\n",
    "# Set parameters for the algorithm.\n",
    "# Here, we limit the number of iterations to 10.\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "# Fit the model to the data.\n",
    "model = lr.fit(df)\n",
    "\n",
    "# Given a dataset, predict each point's label, and show the results.\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example 3: Clickstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resources and References\n",
    "\n",
    "#### A related Meetup this Thursday:\n",
    ">`\"Invited Speaker Series - Cody Koeninger: Fundamentals of Spark and Kafka\"`  \n",
    ">`Thursday, June 23, 2016`  \n",
    ">`6:30 PM to 8:45 PM`  \n",
    "http://www.meetup.com/Austin-ACM-SIGKDD/events/231377005/  \n",
    "\n",
    "#### MOOCs:\n",
    "- \"Data Science and Engineering with Apache Spark\" Series: https://www.edx.org/course/introduction-apache-spark-uc-berkeleyx-cs105x\n",
    "- \"Hadoop Platform and Application Framework\": https://www.coursera.org/learn/hadoop/home/week/5\n",
    "\n",
    "#### Other:\n",
    "- http://spark.apache.org/docs/latest/api/python/\n",
    "- http://spark.apache.org/research.html\n",
    "- http://spark.apache.org/examples.html\n",
    "- http://go.databricks.com/apache-spark-2.0-presented-by-databricks-co-founder-reynold-xin\n",
    "- https://dzone.com/articles/big-data-processing-spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thanks for Coming!\n",
    "\n",
    "Reach out to Meghann Agarwal with any questions or comments on this talk."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
